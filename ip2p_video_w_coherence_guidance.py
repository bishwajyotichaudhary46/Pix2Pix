# -*- coding: utf-8 -*-
"""IP2P Video w/ Coherence Guidance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1inQJPKLOpjB_Bpo0GmboqJWJ1AxzW5Xa

# Introduction

This notebook modifies the InstructPix2Pix pipeline to take an extra image as a reference, with a guidance scale to determine how well the generated image tries to match the reference image.

This is useful for video - we can approximate the next frame using EBSynth (or optical flow or ...) and put this in as a reference. It still allows for some changes, BUT it encourages the model output to be consistent with what we predict given the previous frame, leading to much smoother video outputs.

There are lots of potential improvements to be made, but I figured I'd share since this is already pretty cool to play with.

Please tag me in your creations! @johnowhitaker on Twitter.

# Setup (takes a few minutes)
"""

!pip install -Uqq diffusers fastcore torchvision transformers

!git clone https://github.com/jamriska/ebsynth
!cd ebsynth;nvcc src/ebsynth.cpp src/ebsynth_cpu.cpp src/ebsynth_cuda.cu -I"include" -DNDEBUG -D__CORRECT_ISO_CPP11_MATH_H_PROTO -O6 -std=c++11 -w -Xcompiler -fopenmp -o bin/ebsynth # For GPU

"""# Imports, Definitions, Loading the Pipeline"""

import warnings
warnings.filterwarnings('ignore')
import logging
logging.getLogger().setLevel(logging.CRITICAL)
from diffusers import StableDiffusionInstructPix2PixPipeline
import torch
from PIL import Image
from io import BytesIO
import requests
from torchvision import transforms as tfms
import fastcore.all as fc
from typing import Union, List, Optional,Callable
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_instruct_pix2pix import preprocess
import PIL
import glob, os

device = 'cuda'

# Load the pipeline
model_id = "timbrooks/instruct-pix2pix"
pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None).to(device)

# Uncomment below to save memory if on low ram system
# pipe.enable_attention_slicing()

def resize_img(img, longest_side=768):
    max_size = max(img.size[0], img.size[1])
    scale=1
    if max_size>longest_side:
        scale = longest_side/max_size
    new_w = 8 * ((img.size[0]*scale)//8)
    new_h = 8 * ((img.size[1]*scale)//8)
    img = img.resize((int(new_w), int(new_h)))
    return img

def load_image(url):
    response = requests.get(url)
    img = Image.open(BytesIO(response.content))
    return resize_img(img)

def ims_to_ls(ims):
    with torch.no_grad():
        latent = pipe.vae.encode(ims.to(device)*2-1)
    return 0.18215 * latent.latent_dist.sample()

@fc.patch()
def __call__(
        self:StableDiffusionInstructPix2PixPipeline,
        prompt: Union[str, List[str]],
        image: Union[torch.FloatTensor, PIL.Image.Image],
        num_inference_steps: int = 100,
        guidance_scale: float = 7.5,
        image_guidance_scale: float = 1.5,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = "pil",
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: Optional[int] = 1,
        ref_image = None,  #<<
        ref_guidance_scale=2000,
    ):
        # print('Hello')#<< Check we're using the modified __call__ function :)
        cond_image_latents = ims_to_ls(tfms.ToTensor()(image).unsqueeze(0).half())#<< the target image

        if ref_image is not None:
            ref_image_latents = ims_to_ls(tfms.ToTensor()(ref_image).unsqueeze(0).half())


        # 0. Check inputs
        self.check_inputs(prompt, callback_steps)

        # 1. Define call parameters
        batch_size = 1 if isinstance(prompt, str) else len(prompt)
        device = self._execution_device
        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale > 1.0 and image_guidance_scale >= 1.0
        # check if scheduler is in sigmas space
        scheduler_is_in_sigma_space = hasattr(self.scheduler, "sigmas")

        # 2. Encode input prompt
        with torch.no_grad(): #<<
            text_embeddings = self._encode_prompt(
                prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt
            )

            # 3. Preprocess image
            image = preprocess(image)
            height, width = image.shape[-2:]

            # 4. set timesteps
            self.scheduler.set_timesteps(num_inference_steps, device=device)
            timesteps = self.scheduler.timesteps

            # 5. Prepare Image latents
            image_latents = self.prepare_image_latents(
                image,
                batch_size,
                num_images_per_prompt,
                text_embeddings.dtype,
                device,
                do_classifier_free_guidance,
                generator,
            )

            # 6. Prepare latent variables
            num_channels_latents = self.vae.config.latent_channels
            latents = self.prepare_latents(
                batch_size * num_images_per_prompt,
                num_channels_latents,
                height,
                width,
                text_embeddings.dtype,
                device,
                generator,
                latents,
            )

            # 7. Check that shapes of latents and image match the UNet channels
            num_channels_image = image_latents.shape[1]
            if num_channels_latents + num_channels_image != self.unet.config.in_channels:
                raise ValueError(
                    f"Incorrect configuration settings! The config of `pipeline.unet`: {self.unet.config} expects"
                    f" {self.unet.config.in_channels} but received `num_channels_latents`: {num_channels_latents} +"
                    f" `num_channels_image`: {num_channels_image} "
                    f" = {num_channels_latents+num_channels_image}. Please verify the config of"
                    " `pipeline.unet` or your `image` input."
                )

            # 8. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
            extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        # 9. Denoising loop
        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                # Expand the latents if we are doing classifier free guidance.
                # The latents are expanded 3 times because for pix2pix the guidance\
                # is applied for both the text and the input image.
                latent_model_input = torch.cat([latents] * 3) if do_classifier_free_guidance else latents

                # concat latents, image_latents in the channel dimension
                scaled_latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)
                scaled_latent_model_input = torch.cat([scaled_latent_model_input, image_latents], dim=1)

                # predict the noise residual
                with torch.no_grad(): #<<
                    noise_pred = self.unet(scaled_latent_model_input, t, encoder_hidden_states=text_embeddings).sample

                # Hack:
                # For karras style schedulers the model does classifer free guidance using the
                # predicted_original_sample instead of the noise_pred. So we need to compute the
                # predicted_original_sample here if we are using a karras style scheduler.
                if scheduler_is_in_sigma_space:
                    step_index = (self.scheduler.timesteps == t).nonzero().item()
                    sigma = self.scheduler.sigmas[step_index]
                    noise_pred = latent_model_input - sigma * noise_pred

                # perform guidance
                if do_classifier_free_guidance:
                    noise_pred_text, noise_pred_image, noise_pred_uncond = noise_pred.chunk(3)
                    noise_pred = (
                        noise_pred_uncond
                        + guidance_scale * (noise_pred_text - noise_pred_image)
                        + image_guidance_scale * (noise_pred_image - noise_pred_uncond)
                    )

                # Hack:
                # For karras style schedulers the model does classifer free guidance using the
                # predicted_original_sample instead of the noise_pred. But the scheduler.step function
                # expects the noise_pred and computes the predicted_original_sample internally. So we
                # need to overwrite the noise_pred here such that the value of the computed
                # predicted_original_sample is correct.
                if scheduler_is_in_sigma_space:
                    noise_pred = (noise_pred - latents) / (-sigma)

                ## GUIDANCE #<<
                if ref_image is not None:
                    latents = latents.detach().requires_grad_()
                    pred_x0 = self.scheduler.step(noise_pred, t, latents).pred_original_sample
                    loss = (ref_image_latents - pred_x0).pow(2).mean() * ref_guidance_scale
                    cond_grad = torch.autograd.grad(loss, latents)[0]
                    latents = latents.detach() - cond_grad # Technically should be some scaling here ;)

                # compute the previous noisy sample x_t -> x_t-1
                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample

                # call the callback, if provided
                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        callback(i, t, latents)

        # 10. Post-processing
        with torch.no_grad():
            image = self.decode_latents(latents)

        return self.numpy_to_pil(image)[0] # Just return the image

!pwd

"""# Preparing a Video

Upload your file and edit the video_file variable. Example video linked in the comment. This splits the video into frames, resized to the specified width and height.
"""

# Specify image size:
width = 640 #@param
height = 360 #@param
size_str = f'{width}x{height}'

# Specify video file:
video_file = 'dancing.mp4' #@param
# By Anna Shvets, https://www.pexels.com/video/woman-with-headphones-dancing-4316089/

# Split source video into frames:
!rm -rf source_frames
!mkdir -p source_frames
!ffmpeg -i $video_file -s $size_str -v 1 source_frames/%05d.jpeg



"""# Test Edit on a Single Frame

Nice way to figure out what you want before wasting time on hundreds of frames :)
"""

instruction = 'Make her a golden statue'
pipe(instruction, Image.open(''),
     num_inference_steps=30, image_guidance_scale=1.6)

"""# Apply to all frames, with guidance

Set max_frames_to_style if you don't want to process the whole video (good for tests)

Set process_every to 2 to process every second frame - nice when later using 12fps. Again speeds up testing.

ref_giodance_scale determines the strength of the guidance. Experiment, a good range to try is 30-300. More => smoother, sometimes even a much higher value works.

num_inference_steps can be set higher but will give slower processing times.

image_guidance_scale is how stringly IP2P will follow the image, higher -> less large changes to overall structure in general, tweak between 1 and 3 as a starting point.
"""

instruction = 'Make her a golden statue' #@param
max_frames_to_style = 60 #@param
process_every = 1 #@param
ref_guidance_scale = 250 #@param
image_guidance_scale = 1.6 #@param
num_inference_steps = 30 #@param
seed = 42 #@param

# Prepare a folder to save results
!rm -rf result_frames
!mkdir -p result_frames

# Load the frame filenames
frames = sorted(glob.glob('source_frames/*.jpeg'))
frames = frames[:max_frames_to_style*process_every]

for i, f in enumerate(frames):
    if i % process_every != 0: continue
    generator = torch.Generator("cuda").manual_seed(seed)
    if i == 0: # First frame no guidance
        im = pipe(instruction, Image.open(f), num_inference_steps=30,generator=generator, image_guidance_scale=image_guidance_scale)
    else:
        # Predict the frame based w/ EBSynth
        style_filename = f'result_frames/{i+1-process_every:05d}.jpeg' # Previous output (it starts at 1)
        guide_filename = frames[i-process_every] # Previous input
        next_frame = f
        command = f'./ebsynth/bin/ebsynth  -style {style_filename} -guide {guide_filename} {next_frame}  -output /content/temp.jpeg'
        os.system(command)
        im = pipe(instruction, Image.open(f), ref_image=Image.open('temp.jpeg'),
                  ref_guidance_scale=ref_guidance_scale, num_inference_steps=num_inference_steps, generator=generator,  image_guidance_scale=image_guidance_scale)

    im.save(f'result_frames/{1+i//process_every:05d}.jpeg')

"""# Turn the resulting frames into a video

Edit `-framerate 12` if you want to use a different framerate, in this example I liked the slow-motion effect :)
"""

!ffmpeg -y -f image2 -framerate 12 -i result_frames/%05d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p output_video.mp4

# Download output_video.mp4 and enjoy

"""# Comments

On an A100 this takes ~1.5 seconds per frame (30 steps) which makes it reasonably fast compared to some of the video techniques of yore!

Tweak the image_guidance_scale for different effects - higher scales mean more rigid adherence to structure.

I recommend experimenting with a few seconds' worth of frames until you have settings dialled in - then you can up the resolution and style a whole video.
"""